native_include "sisl/utility/non_null_ptr.hpp";

namespace iomgrcfg;

attribute "hotswap";
attribute "deprecated";

table DriveInterface {
    /* Number of batched io limit for SPDK request */
    num_batch_io_limit: uint32 = 64 (hotswap); 

    // io watchdog on/off
    io_watchdog_timer_on: bool = false;

    // io watchdog check outstanding I/O hang periodically
    io_watchdog_timer_sec: uint64 = 300 (hotswap);

    // io timeout limit in seconds
    io_timeout_limit_sec: uint64 = 60 (hotswap);

    retry_timeout_us: uint32 = 1000 (hotswap);

    // Not applicable for Spdk devices
    zeros_by_ioctl: bool = false;

    partial_read_max_resubmit_cnt: uint32 = 256 (hotswap); // max resubmit cnt of io in case of partial read, only valid for uring
                                                           // TODO: this value should be set by consumer of iomgr, which should be (max_io_size / physical_page_sz) in worst case

    max_resubmit_cnt: uint32 = 3 (hotswap); // max resubmit cnt of io in case of error 
}

table PoolEntry {
    // Size of each mempool entry
    size : uint64; 

    // Percentage of total pool memory
    percent : double; 
}

table IOMemory {
    // General Memory size available for the app in MiB
    app_mem_size_mb: uint64 = 0;
    
    // If its a hugepage based available memory for hugepages (to do IO)
    hugepage_size_mb: uint64 = 0;

    // Percentage of memory to be filled by app before we ask underlying mem allocator to free it up
    soft_mem_release_threshold: uint32 = 85;

    // Percentage of memory to be filled by app before we force underlying mem allocator to free it up
    aggressive_mem_release_threshold: uint32 = 95;

    // Rate of memory release rate to the underlying mem allocator
    mem_release_rate: uint32 = 8;

    // Frequency in count of alloc/free to check if memory limit is exceeded
    limit_check_freq: uint32 = 1000;

    // Array of pool sizes
    pool_sizes : [PoolEntry];
}

table Thread {
    num_workers: uint32 = 2;

    num_fibers: uint32 = 4;
}

table Poll {
    // Frequency in milliseconds that poll will wake up during non-IO periods. -1 would mean never, 0 would
    // mean run always in tight loop
    force_wakeup_by_time_ms: int = 10;

    // The max ios after which thread runs in a tight loop
    tight_loop_after_io_max: uint32 = 4;

    // Upon every outstanding io in the reactor thread, the wait time is reduced by the factor of this setting
    // So if io_wakeup_interval_max = 4, then when no IO it will wait for "quiet_wakeup_interval_ms", then upon
    // first io, it will wait for "io_wakeup_interval_max", then next outstanding io, it will wait for 
    // "io_wakeup_interval_max" * "io_wakeup_interval_decay_percentage" and so on.. until wait time becomes zero
    // where it will run tight loop.
    force_wakeup_by_io_decay_factor : float = 0.5;

    //io_wakeup_interval_decay_percentage : float = 0.5;
   
    // Backoff of the loop. It is the minimum time it backsoff. Backoff starts with this value. We are not making
    // this value hotswappable because, in every tight loop it will have to get the value, which is not ideal
    backoff_delay_min_us : uint64 = 2;

    // Everytime we progressively delays, increase the next delay with this factor. By default it doubles
    // the previous delay
    backoff_delay_increase_factor: float = 2.0 (hotswap); 

    // This is maximum delay backoff on every iteration
    backoff_delay_max_us : uint64 = 500 (hotswap);
}

table Message {
    // Max messages processed before yielding for other completions. As of now it is applicable only for EPOLL Reactor
    max_msgs_before_yield: uint32 = 100 (hotswap);
}

table IoEnv {
    cpuset_path: string;

    http_port: uint32 = 5000;
    
    // Turn on security features
    encryption: bool = false;
    authorization: bool = false;
}

table IomgrSettings {
    iomem: IOMemory;
    thread: Thread;
    drive: DriveInterface;
    poll: Poll;
    message: Message;
    io_env: IoEnv;
}

root_type IomgrSettings;
